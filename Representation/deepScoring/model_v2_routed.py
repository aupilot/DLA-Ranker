from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np

class conv_params:
    def __init__(self, layers):
        """
        Class aimed at providing an rapid way to implement different architectures for the
        convolution part of the network in order to test small changes easily. It gathers
        the information needed to build the convolution block in an object that is fed
        at the creation of the model.
        : params layers:
                    ['conv_20_3_VALID', 'conv_30_4_VALID', 'conv_20_4_VALID', 'avgpool_4_VALID']
                    each element of the list is a string, starting either
                    with 'conv' keyword followed by the number of channels
                    and the size of the cubic kernel and the padding (VALID / SAME) or
                    by 'avgpool' for a pooling layer followed by the pooling size.
                    For now every conv layer is followed by a batch normalization
                    layer.
    """
        # Parse the layers list to check for errors
        errors = 0
        for la in layers:
            parse = la.split('_')
            # TODO: improve the errors checking
            if not(parse[0] == 'conv' or parse[0] == 'avgpool'):
                print('ERROR: Anomaly detected while parsing argument :', parse[0],'is not a valid keyword')
                errors += 1

        if not errors == 0:
            raise ValueError(str(errors) + ' error(s) while parsing the argument')
        self.layers = layers

    def get_output_size(self, input_size):
        """
        Used when building the graph of the model
        :param input_size: Number of voxels in one side of the map before convolution bloc
        :return: Number of voxels in one side of the map after convolution operations
        """
        final_size = input_size
        for l in self.layers:
            if l.split('_')[0] == 'conv':
                if l.split('_')[3] == 'VALID':
                    final_size = final_size - int(l.split('_')[2]) + 1
            elif l.split('_')[0] == 'avgpool':
                final_size = int(final_size//int(l.split('_')[1]))
        return final_size

def _weight_variable(name, shape):
    return tf.get_variable(name, shape, tf.float32, tf.truncated_normal_initializer(stddev=0.01))

def _bias_variable(name, shape):
    return tf.get_variable(name, shape, tf.float32, tf.constant_initializer(0.1, dtype=tf.float32))

class ScoringModel:
    def __init__(self,
                 conv_param,
                 num_retype=15,
                 GRID_SIZE=24,
                 batch_norm=True,
                 activation='softplus',
                 final_activation='sigmoid',
                 nb_branches = 5,
                 router = 'base',
                 coarse = False):
        """
        Initialize variable of the model.
        :param conv_param: Object that contains all the parameters used to build different versions of the conv bloc
        :param num_retype: Dimension of the map after dimensionality reduction by the retyper
        :param GRID_SIZE: Number of voxel in one side of the map
        :param batch_norm: True if using batchnorm before activation layers
        :param validation: Activation layer (softplus or elu)
        :param final_activation: Final activation (sigmiod or tanh 'like')
        :param nb_branches: If the model is not routed acording to residue type, numbre of branche in decision tree
        :param router: different versions of the router ('advanced' is a version where at least 0.01 of the data is sent to every branche)
        :param coarse: Bool, True if coarse grained mapping.
        """
        self.conv_param = conv_param
        self.num_retype = num_retype
        self.GRID_SIZE = GRID_SIZE
        self.GRID_VOXELS = self.GRID_SIZE * self.GRID_SIZE * self.GRID_SIZE
        self.coarse = coarse
        self.NB_TYPE = 169
        if coarse:
            self.NB_TYPE=126
        self.num_retype = 3
        self.batch_norm = batch_norm

        self.validation = activation
        self.final_activation = final_activation
        self.nb_branches = nb_branches
        self.router = router

        if self.router == 'routed_res':
            self.nb_branches = 20 # no choice for number of branches if routed res.

    def get_pred(self,
               maps,
               res,
               isTraining):
        """
        Gives the predictions of a given batch of maps. Builds the graph, one retyping layer one conv bloc which
        architecture is defined by self.conv_params and one fully connected bloc.
        :param maps: Maps generated by the protein mapper
        :param res: res is the one hot encoding of the batch's residues types (extracted from the maps meta data).
                    Used if the model is routed according to the residue type
        :param isTraining: True if model is training.
        :return: List of the predictions for a given batch
        """
        input_data = tf.reshape(maps, [-1, self.NB_TYPE, self.GRID_SIZE, self.GRID_SIZE, self.GRID_SIZE])

        # First step reducing data dimensionality
        retyped = self.retype_layer(input_data, self.num_retype, self.NB_TYPE, coarse=self.coarse, name='retype')

        flat0 = self.conv_bloc(retyped, isTraining, self.conv_param, name='main_branche')

        # The router gives a weight to each branche according to the output of the convolution operations
        if not self.router == 'routed_res':
            rout = self.router_bloc(flat0, self.nb_branches, self.router, 'router_1')

        # Each branche makes a prediction that are gathered in outs (the prediction is not normalized)
        outs = []
        for j_ in range(self.nb_branches):
            out_r = self.fc_bloc(flat0, isTraining, name='r_' + str(j_ + 1))
            outs.append(out_r)

        conc = tf.transpose(outs)

        # Each prediction is weighted by either the router or the residue type
        if self.router == 'routed_res':
            out = tf.multiply(conc, res)
        else:
            out = tf.multiply(conc, rout)

        out = tf.reduce_sum(out, axis = 1)
        out = tf.squeeze(out)

        # predictions are normalized between zero and one.
        if self.final_activation == 'tanh':
            return tf.add(tf.tanh(out) * 0.5, 0.5, name="main_output")
        else:
            return tf.sigmoid(out, name="main_output")

    def retype_layer(self, prev_layer, num_retype_, input_dim, name='retype', coarse=False):
        """
        Reduces the dimensionality of the map.
        :param prev_layer: Input data (maps of the proteins)
        :param num_retype_: Dimensionality after retyping
        :param input_: Dimensionality before retyping
        :param name: Name given for the tf variables
        :param coarse: True if mapping coarse grained
        :return: Retyped map
        """
        if coarse:
            dimPerType = 6
            nbBigType = int(self.NB_TYPE / dimPerType)

            retyper = _weight_variable(name + "_" + str(num_retype_), [nbBigType, dimPerType * num_retype_])

            with tf.name_scope(name):
                tf.summary.histogram(name, retyper)

            prev_layer = tf.transpose(prev_layer, perm=[0, 2, 3, 4, 1])
            map_shape = tf.gather(tf.shape(prev_layer), [0, 1, 2, 3])  # Extract the first three dimensions

            map_shape = tf.concat([map_shape, [dimPerType * self.num_retype]], axis=0)
            prev_layer = tf.reshape(prev_layer, [-1, self.NB_TYPE])

            prev_layer = tf.reshape(prev_layer, [-1, nbBigType, dimPerType])
            prev_layer = tf.transpose(prev_layer, perm=[0, 2, 1])

            prev_layer = tf.reshape(prev_layer, [-1, nbBigType])

            prev_layer = tf.matmul(prev_layer, retyper)

            return tf.reshape(prev_layer, map_shape)
        else:
            retyper = _weight_variable(name + "_" + str(num_retype_), [input_dim, num_retype_])

            with tf.name_scope(name):
                tf.summary.histogram(name, retyper)

            prev_layer = tf.transpose(prev_layer, perm=[0, 2, 3, 4, 1])
            map_shape = tf.gather(tf.shape(prev_layer), [0, 1, 2, 3])  # Extract the first three dimensions

            map_shape = tf.concat([map_shape, [self.num_retype]], axis=0)
            prev_layer = tf.reshape(prev_layer, [-1, self.NB_TYPE])
            prev_layer = tf.matmul(prev_layer, retyper)

            return tf.reshape(prev_layer, map_shape)

    def conv_layer(self, prev_layer, kernel_size, name='CONV', padding='VALID'):
        """
        One conv layer with biais. the activation layer is not defined here, must be added after.
        :param prev_layer: Output of the previous layer
        :param kernel_size: Kernel size (list of integer) [side_size, side_size, side_size, nb_channels_input, nb_channels_output]
        :param name: Name given to the operations in that layer. must bu unique
        :param padding: Padding, SAME or VALID
        :return: output layer (conv + biais)
        """
        kernelConv = _weight_variable("weights_" + name + "_" + str(self.num_retype), kernel_size)
        prev_layer = tf.nn.conv3d(prev_layer, kernelConv, [1, 1, 1, 1, 1], padding=padding, name = name)
        biasConv = _bias_variable("biases_" + name + "_" + str(kernel_size[3]), kernel_size[-1])

        with tf.name_scope(name):
            tf.summary.histogram("weights_" + name, kernelConv)
            tf.summary.histogram("biases_" + name, biasConv)

        return prev_layer + biasConv

    def activation_normalization_layer(self, input_vector, batch_norm, validation, isTraining, name='activ_norma_'):
        """
        Activation layer, with optional batch noramlization
        :param input_vector: Output of the previous layer
        :param batch_norm: Bool, True if you want to enable batch noramlization
        :param validation: string, between elu and softplus
        :param isTraining: True if the network is in its training phase (useful for batch normalization)
        :param name: Unique name given to operation in this layer
        :return:
        """
        if batch_norm:
              input_vector = tf.layers.batch_normalization(input_vector, training=isTraining, name = name)
        if validation == 'softplus':
            return tf.nn.softplus(input_vector, name="softplus")
        elif validation == 'elu':
            return tf.nn.elu(input_vector, name="elu")

    def fc_layer(self, input_vector, output_size, bias, name):
        """
        Fully connected layer
        :param input_vector: Output of the previous layer
        :param output_size: dimension of the output vector
        :param bias: True if you want biais in the layer
        :param name: Unique name to give to the operations in this layers
        :return: Output of fully connected layer
        """
        # exctract the input size from previous layer
        input_size = input_vector.get_shape().as_list()[1]
        weightsLinear = _weight_variable("weights_" + name + "_" + str(self.num_retype), [input_size, output_size])
        prev_layer = tf.matmul(input_vector, weightsLinear)
        if bias:
            biasLinear = _bias_variable("biases_" + name + "_" + str(self.num_retype), [output_size])

        with tf.name_scope(name):
            tf.summary.histogram("weights_" + name, weightsLinear)
            if bias:
                tf.summary.histogram("biases_" + name, biasLinear)
        if bias:
            return prev_layer + biasLinear
        else:
            return prev_layer

    def router_bloc(self, input_vector, nb_route = 2, router='base', name = 'router'):
        """
        Bloc that takes the result of convolutionnal operations and predicts a weight to give to each branch.
        Small network of two fully connected layers with softplus activation
        :param input_vector: Output of conv bloc
        :param nb_route: Number of branches to choose between
        :param router: If different versions of the router qre implemented, allows to choose the one to use.
                       for now, 'advanced' allows to send a fraction of the predictions made by each branche to the final
                       result to avoid the case where only one branch is selected and help training.
        :param name: Unique name given to the operations in these layers
        :return: A vector containing a weight for each branche
        """
        # exctract the input size from previous layer
        input_size = input_vector.get_shape().as_list()[1]
        out_size = input_size // 2
        weightsLinear = _weight_variable("weights_" + name, [input_size, out_size])
        biasLinear = _bias_variable("biases_" + name, [out_size])

        routes = tf.matmul(input_vector, weightsLinear) + biasLinear

        routes = tf.nn.softplus(routes, name="softplus_" + name)

        weightsLinear2 = _weight_variable("weights2_" + name, [out_size, nb_route])
        biasLinear2 = _bias_variable("biases2_" + name, [nb_route])

        routes = tf.matmul(routes, weightsLinear2) + biasLinear2

        if router=='advanced':
            fraction = 0.01
            return tf.add(tf.nn.softmax(routes)*(1 - fraction), tf.constant(np.ones(nb_route)*fraction, dtype=tf.float32), name='out_' + name)

        return tf.nn.softmax(routes, name='out_' + name)

    def fc_bloc(self, input, train, name = 'r_'):
        """
        Fully connected bloc. Two fully conected layer one activation + batch norm after first layer, none after second
        :param input: Output of the previous bloc
        :param train: True if network is in training phase : useful for batch normalization so that it only adjusts on training data
        :param name: Unique name for operations in this layer
        :return: One prediction (not normalized)
        """
        LINEAR1_OUT = 64
        out_l1 = self.fc_layer(input, LINEAR1_OUT, bias=True, name='linear_1_' + name)
        out_l1 = self.activation_normalization_layer(out_l1, self.batch_norm, self.validation, train,
                                                    name='act_norm_4_' + name)

        out = self.fc_layer(out_l1, 1, False, 'Linear_2_' + name)

        return tf.squeeze(out)

    def conv_bloc(self, input, train, params_conv, name = 'r_'):
        """
        Conv operation, dicted by the conv params
        :param input: output of retype layer, retyped 3D map
        :param train:  True if network is in training phase : useful for batch normalization so that it only adjusts on training data
        :param params_conv: list of strings that discribe the architecture of the bloc
        :param name: unique name for operations in the bloc
        :return: flattened vector, result of conv operations
        """
        id = 0
        # extract the nb of channel of the input vector
        last_channel =  input.get_shape().as_list()[-1]

        for l_i, layer in enumerate(params_conv.layers):
            id += 1
            # For each string in the conv_params list, add a layer that correspond to the specified one conv or maxpool
            if layer.split('_')[0] == 'conv':
                # exctract the hyperparameters from the string
                k_side = int(layer.split('_')[2])
                input_channel, output_channel = last_channel, int(layer.split('_')[1])
                # keep output number of channel in memory for next layer
                last_channel = output_channel
                kernel_params = [k_side, k_side, k_side, input_channel, output_channel]
                input = self.conv_layer(input, kernel_params, name='CONV_'+ str(id)+'_'+name, padding=layer.split('_')[3])
                input = self.activation_normalization_layer(input, self.batch_norm, self.validation, train, name='act_norm_' + str(id) + '_' + name)

            elif layer.split('_')[0] == 'avgpool':
                pool_size = int(layer.split('_')[1])
                if not pool_size == 0:
                    input = tf.nn.avg_pool3d(input,
                                            [1, pool_size, pool_size, pool_size, 1],
                                            [1, pool_size, pool_size, pool_size, 1],
                                            padding='VALID')
        final_side_size = params_conv.get_output_size(self.GRID_SIZE)
        nb_dimout =  final_side_size * final_side_size * final_side_size * last_channel
        return tf.reshape(input, [-1, nb_dimout])

    def compute_loss(self, scores, cad_score):
        """
        Compute loss, l2 distance
        :param scores: List of predicted scores for batch
        :param cad_score: List of ground truth scores
        :return: list of l2 distance between predicted scores and gt scores
        """
        return tf.square(scores - cad_score, name='loss')

    def train(self, loss, learning_rate):
        """
        Train operations when building graph. With adam optimizer
        :param loss: output of loss function
        :param learning_rate: Learning rate
        :return: training operation output of Optimizer.minimize()
        """
        optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9)
        # optimizer = tf.train.RMSPropOptimizer(learning_rate, decay = 0.999)
        global_step = tf.Variable(0, name='global_step', trainable=False)
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, global_step=global_step, name='train_op')

        return train_op